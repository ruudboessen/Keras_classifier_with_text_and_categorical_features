{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-16T18:53:04.116949",
     "start_time": "2019-11-16T18:53:02.623671"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Embedding, LSTM, Dropout, Dense, Reshape, Concatenate\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load data and select only tweets with positive and negative sentiment: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('twitter-airline-sentiment.csv')\n",
    "data = data[['text','airline','airline_sentiment']]\n",
    "\n",
    "data = data[data['airline_sentiment'].str.contains(\n",
    "    '|'.join(['positive','negative']))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train / Test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-16T17:53:02.651Z"
    }
   },
   "outputs": [],
   "source": [
    "p_train = 0.8 # proportion in train.\n",
    "inTrain = np.random.choice(len(data), int(p_train*len(data)), replace=False)\n",
    "train = data.iloc[inTrain].reset_index(drop=True)\n",
    "valid = data.drop(inTrain).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T11:26:16.377263",
     "start_time": "2019-11-11T11:26:16.367797"
    }
   },
   "source": [
    "##### Tokenizer for text feature:\n",
    "Determine `vocab_size` most frequent words and replace their occurrence with a number. Do this for the first `seq_length` words per tweet. An n-dimensional embedding is estimated later as part of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-16T17:53:02.654Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "vocab_size = 5000\n",
    "seq_length = 12\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train['text'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoder for categorical feature:\n",
    "Replace each categorical level with a number. Again, an n-dimensional embedding is estimated later as part of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-16T17:53:02.657Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train['airline'])\n",
    "num_classes = len(encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Apply pre-processing steps (i.e. tokenizer and encoder) to train and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-16T17:53:02.658Z"
    }
   },
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(train['text'].values)\n",
    "X_train_LSTM = np.array(pad_sequences(sequences, \n",
    "                                      maxlen=seq_length, \n",
    "                                      padding='pre', \n",
    "                                      truncating='post'))\n",
    "X_train_FC = encoder.transform(train['airline'])\n",
    "y_train = train['airline_sentiment'].str.contains('negative').astype(int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-16T17:53:02.661Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 9232 rows do not contain at least one of the words in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "print(str(sum(np.sum(X_train_LSTM, axis=1)==0))+' of '+str(len(X_train_LSTM))+\n",
    "      ' rows do not contain at least one of the words in the vocabulary.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-16T17:53:02.664Z"
    }
   },
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(valid['text'].values)\n",
    "X_valid_LSTM = np.array(pad_sequences(sequences, \n",
    "                                     maxlen=seq_length, \n",
    "                                     padding='pre', \n",
    "                                     truncating='post'))\n",
    "X_valid_FC = encoder.transform(valid['airline'])\n",
    "y_valid = valid['airline_sentiment'].str.contains('negative').astype(int).values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Specify model:\n",
    "The model contains two components that are concatenated. The first component includes an Embedding layer, a LSTM layer and a Dropout layer and receives the tokenized text as its input. The second component includes an Embedding layer and a Dropout layer and receives the encoded categorical data as its input. After concatenation a Dense layer is added to map the outcomes of both components to the final target (e.g. sentiment; positive / negative).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-16T17:53:02.668Z"
    }
   },
   "outputs": [],
   "source": [
    "LSTM_embed_dim = 32 \n",
    "LSTM_input = Input(shape=(seq_length,), name='LSTM_input')\n",
    "LSTM_embed = Embedding(input_dim=vocab_size, input_length=seq_length, \n",
    "                       output_dim=LSTM_embed_dim, name='LSTM_embed')(LSTM_input)\n",
    "LSTM_layer = LSTM(64, name='LSTM_output')(LSTM_embed)\n",
    "LSTM_regul = Dropout(rate=0.5, name='LSTM_dropout')(LSTM_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-16T17:53:02.670Z"
    }
   },
   "outputs": [],
   "source": [
    "FC_embed_dim = 3\n",
    "FC_input = Input(shape=(1,), name='FC_input')\n",
    "FC_embed = Embedding(input_dim=num_classes, input_length=1, \n",
    "                     output_dim=3, name='FC_embed')(FC_input)\n",
    "FC_layer = Reshape(target_shape=(3,), name='FC_output')(FC_embed)\n",
    "FC_regul = Dropout(rate=0.5, name='FC_dropout')(FC_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-16T17:53:02.671Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "LSTM_input (InputLayer)         (None, 12)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FC_input (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "LSTM_embed (Embedding)          (None, 12, 32)       160000      LSTM_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "FC_embed (Embedding)            (None, 1, 3)         18          FC_input[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "LSTM_output (LSTM)              (None, 64)           24832       LSTM_embed[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "FC_output (Reshape)             (None, 3)            0           FC_embed[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "LSTM_dropout (Dropout)          (None, 64)           0           LSTM_output[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "FC_dropout (Dropout)            (None, 3)            0           FC_output[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 67)           0           LSTM_dropout[0][0]               \n",
      "                                                                 FC_dropout[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "full_dense (Dense)              (None, 1)            68          concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 184,918\n",
      "Trainable params: 184,918\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "full_merge = Concatenate()([LSTM_regul, FC_regul])\n",
    "full_dense = Dense(1, activation='sigmoid', name='full_dense')(full_merge) \n",
    "full_model = Model(inputs=[LSTM_input, FC_input], outputs=full_dense)\n",
    "full_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model compilation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-16T17:53:02.673Z"
    }
   },
   "outputs": [],
   "source": [
    "full_model.compile(loss='binary_crossentropy',\n",
    "                   optimizer='adam',\n",
    "                   metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model fitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-16T17:53:02.674Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9232 samples, validate on 2309 samples\n",
      "Epoch 1/10\n",
      "9232/9232 [==============================] - 3s 343us/step - loss: 0.4006 - acc: 0.8384 - val_loss: 0.2768 - val_acc: 0.8874\n",
      "Epoch 2/10\n",
      "9232/9232 [==============================] - 2s 192us/step - loss: 0.2222 - acc: 0.9182 - val_loss: 0.2348 - val_acc: 0.9125\n",
      "Epoch 3/10\n",
      "9232/9232 [==============================] - 2s 181us/step - loss: 0.1606 - acc: 0.9430 - val_loss: 0.2491 - val_acc: 0.9117\n",
      "Epoch 4/10\n",
      "9232/9232 [==============================] - 2s 210us/step - loss: 0.1227 - acc: 0.9565 - val_loss: 0.2760 - val_acc: 0.9151\n",
      "Epoch 5/10\n",
      "9232/9232 [==============================] - 2s 202us/step - loss: 0.0993 - acc: 0.9656 - val_loss: 0.3040 - val_acc: 0.9021\n",
      "Epoch 6/10\n",
      "9232/9232 [==============================] - 2s 180us/step - loss: 0.0841 - acc: 0.9692 - val_loss: 0.3092 - val_acc: 0.9000\n",
      "Epoch 7/10\n",
      "9232/9232 [==============================] - 2s 182us/step - loss: 0.0700 - acc: 0.9766 - val_loss: 0.3704 - val_acc: 0.9017\n",
      "Epoch 8/10\n",
      "9232/9232 [==============================] - 2s 181us/step - loss: 0.0555 - acc: 0.9834 - val_loss: 0.3807 - val_acc: 0.8948\n",
      "Epoch 9/10\n",
      "9232/9232 [==============================] - 2s 191us/step - loss: 0.0444 - acc: 0.9856 - val_loss: 0.4365 - val_acc: 0.8922\n",
      "Epoch 10/10\n",
      "9232/9232 [==============================] - 2s 214us/step - loss: 0.0408 - acc: 0.9866 - val_loss: 0.4757 - val_acc: 0.8956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f52244f7a90>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_model.fit([X_train_LSTM, X_train_FC], y_train, epochs=10, batch_size=64,\n",
    "               validation_data=([X_valid_LSTM, X_valid_FC], y_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
